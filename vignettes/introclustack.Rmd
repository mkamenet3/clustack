---
title: "Introduction to clustack"
author: "M.Kamenetsky"
# date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
# fig_caption: yes
#output: pdf_document
vignette: >
  %\VignetteIndexEntry{Introduction to clustack}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

Please report any issues [here](https://github.com/mkamenet3/clustack/issues).

First, we load the `R` packages we will be using. We will need `clustack`:

```{r}
library(clustack)
```


If you haven't already, you can install `clustack` directly from GitHub:

```{r, eval=FALSE}
library("devtools")
devtools::install_github("mkamenet3/clustack")
```

We will also need the  `clusso`, `MASS`,`RColorBrewer`, and `dplyr` packages.

```{r}
library(clusso) #can also be installed with devtools::install_github("mkamenet3/clusso")
library(RColorBrewer)
library(tidyr)
library(dplyr)
```

## Using `clustack`

In this example, we will use simulated data of breast cancer incidence counts across 208 prefects (municipalities) in Japan across 5 time periods. These data are based on original incidence counts, but we have added noise to these data.


### Prepare Data

We load four data sets that come with the `clustack` package (these are the same data available from the `clusso` package):


```{r}
#load data
data("jbc2")
data("utmJapan")
```


To explore the 4 data sets, we apply the `head()` function to each of the data sets.

1. `utmJapan`: data set containing a unique identifier for each prefect centroid (*id*), x-coordinate in UTM (*utmx*), and y-coordinate in UTM (*utmy*).
1. `jbc2`: data set containing observed counts for each cell $i$ at time $t$ (*Observed*), expected counts for each cell $i$ at time $t$ (*Expected*), and time period ($t=1, \dots,T=5$). In this dataset, a fake cluster has been placed in center 150, with a radius of < 15km, in periods 3 through 5. A relative risk of 2 was used to create this cluster.



```{r}
#inspect
head(utmJapan)
head(jbc2)
```


### Set Global Parameters


```{r}
cases <- jbc2$death
expected <- jbc2$expdeath
x <- utmJapan$utmx/1000 #easting
y <- utmJapan$utmy/1000 #northing

maxclust <- 15 #maximum number of clusters in the study region
rMax <- 20 #maximum radius for each potential cluster
Time <- 5 #number of time periods in the data

```

# Stacking by Potential Cluster


## Cluster Identification and Estimation

<!-- The main function for cluster detection using stacking is `detectclusters()`. -->

We first need to set up several objects. First, we need to create the set of potential clusters using the `clusters2df()` function from `clusso`:

```{r}
potentialclusters <- clusso::clusters2df(x,y,rMax, utm = TRUE, length(x))
str(potentialclusters)
```

`potentialclusters` is a data frame where each row is a potential cluster. `center` and `centerID` are equivalent and are the cell ID number in the dataset, `x` is the x-coordinate of the cell centroid (easting, in $km$), `y` is the y-coordinate of the cell centroid (northing, in $km$), `r` is the radius, `n` is the number of cells in the potential cluster, and `last` is the cell ID of the last cell inside the potential cluster for the given radius.


Next we create `Ex`, which are the expected counts and `Yx` which are the observed counts.

```{r}
Ex <- as.vector(matrix(jbc2$Expected, ncol=Time, byrow=FALSE))
Yx <- as.vector(matrix(jbc2$Observed, ncol=Time, byrow=FALSE))
```

```{r, eval=FALSE, echo=FALSE}
Ex <- as.vector(matrix(test$Ex, ncol=Time, byrow=FALSE))
Yx <- as.vector(matrix(test$Yx, ncol=Time, byrow=FALSE))


```


Importantly, we create a large sparse matrix of single potential clusters (`sparseMAT`), which the rows are the space-time locations and columns are the respective potential clusters.

```{r}
#create giant sparse design matrix (single potential clusters)
(numCenters <- length(unique(potentialclusters$center)))
sparseMAT <- clusso::spacetimeMat(potentialclusters, numCenters, Time) 
dim(sparseMAT)
```

In this example, we are going to assume overdispersion and estimate it separately:

```{r}
#calculate overdispersion estimate
offset_reg <- glm(Yx ~ as.factor(rep(c("1","2","3","4","5"),  each=length(Ex)/Time)) + offset(log(Ex)),
                  family=quasipoisson)
overdisp.est <- clusso::overdisp(offset_reg, sim=FALSE, overdispfloor = TRUE)
overdisp.est
```


We are next ready to use `detectclusters()`. The arguments the function are: the large sparse matrix `sparseMAT`, expected counts `Ex`, observed counts `Yx`, the number of unique locations (`length(x)=208`), Time periods, and maximum number of clusters to be considered; `byloc=FALSE` gives us detection by potential cluster. To detect by location, set `byloc=TRUE`. We set `model="poisson"` and `overdisp.est` equal to our overdispersion estimate calculated above.


```{r}
#run clustack by PC
clustack_pc <-detectclusters(sparseMAT, Ex, Yx, numCenters, Time, maxclust, 
                              byloc = FALSE, model="poisson", overdisp.est = overdisp.est)
```


The output of `detectclusters()` prints out the type of model specified. If `overdisp.est=NULL`, then this would correspond to a Poisson model. The cluster IDs (which correspond to the columns of `sparseMAT`) are then printed in the order that they are detected, followed by the overdispersion estimate. Finally, the number of clusters selected by BIC, AIC, and AICc are printed.

The object `clustack_pc` is a large list of 11 elements:

```{r}
str(clustack_pc)
```


The elements for `clustack_pc` are:

- `wLambda`: a matrix of stacked relative risk estimates. Rows correspond to each 1 through `maxclust` cluster and columns are each space-time location.
- `LRT`: is a vector of loglikelihoods starting with the null model (first element) followed by each cluster up to `maxclust`.
- `selection.bic`, `selection.aic`,`selection.aicc`: the number of clusters identified by each respective selection criterion.
- `selection.bic_forceid`, `selection.aic_forceid`,`selection.aicc_forcid`: the number of clusters identified by each respective selection criterion when clustack is forced to identify a cluster.
- `wtMAT`: a matrix of likelihood-based weights. Rows correspond to each potential cluster and columns correspond to each cluster identified from 1 to `maxclust`.
- `maxid`: The ID of either the potential cluster (if using detection by potential cluster) or location (if using detection by location).
- `Lambda_dense`: Large matrix of single cluster estimates. Rows correspond to each potential cluster and columns correspond to each space-time location.



### Plotting


There are several ways to visualize our results. Because we already have the boundary and map data, we create a quick function called `plotmap()`:

```{r}
plotmap <- function(res, pdfname=NULL, genpdf=TRUE, maxrr=2){
    if(!is.null(maxrr)){
        maxrr=maxrr
    }
    else{
        maxrr=2
    }
    cluster_ix <- redblue(log(maxrr *  pmax(1/maxrr, pmin(res, maxrr)))/log(maxrr^2))
    colors_0 <- matrix(cluster_ix, ncol=5, byrow = FALSE)
    if(genpdf==TRUE){
        pdf(pdfname, height=11, width=10)    
    }
    #Time 1
    par(fig=c(0,.2,.6,1), mar=c(.5,0.5,0.5,0))
    plot(japan.poly2,type='n',asp=1,axes=F,xlab='',ylab='')
    polygon(japan.poly2,col=colors_0[,1] ,border=F)
    segments(japan.prefect2$x1,japan.prefect2$y1,japan.prefect2$x2,japan.prefect2$y2)
    
    #Time 2
    par(fig=c(0.2,.4,.6,1), mar=c(.5,0.5,0.5,0), new=T)
    plot(japan.poly2,type='n',asp=1,axes=F,xlab='',ylab='')
    polygon(japan.poly2,col=colors_0[,2] ,border=F)
    segments(japan.prefect2$x1,japan.prefect2$y1,japan.prefect2$x2,japan.prefect2$y2)
    
    #Time 3
    par(fig=c(0.4,.6,.6,1), mar=c(.5,0.5,0.5,0), new=T)
    plot(japan.poly2,type='n',asp=1,axes=F,xlab='',ylab='')
    polygon(japan.poly2,col=colors_0[,3] ,border=F)
    segments(japan.prefect2$x1,japan.prefect2$y1,japan.prefect2$x2,japan.prefect2$y2)
    
    #Time 4
    par(fig=c(0.6,.8,.6,1), mar=c(.5,0.5,0.5,0), new=T)
    plot(japan.poly2,type='n',asp=1,axes=F,xlab='',ylab='')
    polygon(japan.poly2,col=colors_0[,4] ,border=F)
    segments(japan.prefect2$x1,japan.prefect2$y1,japan.prefect2$x2,japan.prefect2$y2)
    
    #Time 5
    par(fig=c(0.8,1,.6,1), mar=c(.5,0.5,0.5,0), new=T)
    plot(japan.poly2,type='n',asp=1,axes=F,xlab='',ylab='')
    polygon(japan.poly2,col=colors_0[,5] ,border=F)
    segments(japan.prefect2$x1,japan.prefect2$y1,japan.prefect2$x2,japan.prefect2$y2)
    
    #legend
    par(fig=c(.35,.75,0,.1), new=T)
    plot(1, xlim=c(0.6,1.5), ylim=c(0.2,1), axes=F, type='n',  xlab="", ylab="")
    len <- length(redblue(15:50/50))
    rect(seq(.6,1.4,length=len)[-len],.5,seq(.65,1.4,length=len)[-1],.62,col=redblue(15:50/50),border=F)
    text(seq(.6,1.4,length=7),rep(.45,5),seq(1/maxrr,maxrr,length.out=7),srt=330,adj=0)   
    if(genpdf==TRUE){
        dev.off()    
    }
}
```

We visualize the detection by (Q)BIC by plotting the stacked relative risk estimates (`wLambda`) for the number of selected clusters. We set `genpdf=FALSE` to not generate a pdf and set the maximum relative risk on the map to be 1.5 and the minimum relative risk to be 0.5:


```{r, fig.height=5, fig.width=10}
#Detection by (Q)BIC
plotmap(clustack_pc$wLambda[clustack_pc$selection.bic,],genpdf = FALSE, maxrr=2)
```


We can extract summaries of the stacked relative risk estimates for detection by (Q)BIC:

```{r}
table(round(clustack_pc$wLambda[clustack_pc$selection.bic,],2))

```

Next, we create a table of the stacked relative risk estimates as detected by (Q)BIC and (Q)AIC:

```{r}
### Make table ----
bicsum_pc <- as.data.frame(round(clustack_pc$wLambda[clustack_pc$selection.bic,],2)) %>% 
    mutate(bic=`round(clustack_pc$wLambda[clustack_pc$selection.bic, ], 2)`) %>%
    filter(round(bic,2) !=1) %>%
    summarize(min = min(bic),
              max = max(bic),
              Q1 = round(quantile(bic, probs=0.25),2),
              median= median(bic),
              Q3 = round(quantile(bic, probs=0.75),2),
              mean=round(mean(bic),2))



bicsum_pc %>%
    tibble::rownames_to_column('IC') %>%
    kableExtra::kbl(caption="Clustack - by Potential Cluster: Stacked Relative Risk Estimates") %>%
    kableExtra::kable_paper("hover", full_width = F) 

```

# Estimate cell-specific bounds

With `clustack`, we are also able to calculate confidence bounds for specific cells using 5 methods:

- `outnonstack`: Confidence bounds that do not take into consideration the model selection uncertainty from stacking. Wald-type bounds.
- `outnonstack_asymp`:  Confidence bounds that do not take into consideration the model selection uncertainty from stacking, assuming asymptotic Wald-type bounds.
- `outbuck`: Confidence bounds based on Buckland et al.
- `outmaw2`: Confidence bounds based on Burnham and Anderson.
- `outMATA`: Confidence bounds based on model-averaged tail intervals (Turek et al.)

For each of these methods, natural-log transformed and non-transformed bounds and estimates are available, as well as timings.

For this example, we will explore confidence bounds for 5 cells in periods 1 and 3: the cluster center cell (time period 1: cell 150, time period 3: cell 566), the near center cell (time period 1: cell 138, time period 3: cell 554), the border inside cell (time period 1: cell 139, time period 3: cell 555), the border outside cell (time period 1: cell 123, time period 3: cell 539), and the far outside cell (time period 1: cell 55, time period 3: cell 471).



```{r}
#take period 3 only
p3 <- matrix(clustack_pc$wLambda[clustack_pc$selection.bic,], ncol=5)[,3]
#150=center; 138 = near-center; 139 = border-inside; 123=border-outside; 55= far-outside
cellsT1 <- c(150, 138, 139, 123,55)
cellsT3 <- cellsT1 + (208*2)
cells <- c(cellsT1, cellsT3)

```

First, we visualize these cells in the study region in period 3:

```{r, fig.height=5, fig.width=8}
colors2 <- rep("white", 208)
colors2[cells] <- c(brewer.pal(4, "Dark2"), "goldenrod") #select colors

#extract stacked RR estimates
a <- as.data.frame(round(clustack_pc$wLambda[clustack_pc$selection.bic,],2)) %>% 
    mutate(bic=`round(clustack_pc$wLambda[clustack_pc$selection.bic, ], 2)`)
ix <- which(a$bic[417:624]!=1) #identify cells in cluster in period 3 (spans 417:624)
colors <- rep("white", 208) #all other cells in the study region are white
colors[ix] <- "grey60" #other cells in cluster are grey



plot(japan.poly2,type='n',asp=1,axes=F,xlab='',ylab='', main="Cells Selected for Confidence Bounds")
polygon(japan.poly2,col=adjustcolor(colors2,alpha.f=1)  ,border="white")
polygon(japan.poly2,col=adjustcolor(colors,alpha.f=0.5)  ,border="white")
segments(japan.prefect2$x1,japan.prefect2$y1,japan.prefect2$x2,japan.prefect2$y2)
legend("bottomright", inset=-0.01,c("Center", "Near-Center", "Border-Inside",
                                    "Border-Outside", "Far-Outside", "Cluster Cells"),
       fill=c(brewer.pal(4, "Dark2"), "goldenrod", "grey60"), box.lty=0)
```


We are ready to calculate confidence bounds for our selected cells in time period 3. First, we create the object `id.bic_pc`, which is the number of clusters selected:

```{r}
(id.bic_pc <- as.vector(unlist(clustack_pc$selection.bic)))
```

Next, we use the function `calcbounds()` and specify the following arguments: we set `id_ic` to `id.bic_pc`, which is the number of clusters identified by (Q)BIC with stacking by potential cluster; `IC = "bic"` specifies the information criterion used and should correspond to `id_ic`; `res` corresponds to the object created by the stacking step, `clustack_pc`; `byloc` should be set to `FALSE` if stacking by potential cluster and to `TRUE` if stacking by location; `Ex` are the expected counts, `Yx` are the observed counts; `cellsix` is set to `cells` which are the indices of the selected cells for which we want to calculate confidence bounds; `sparsemat` is set to the large sparse matrix, `sparseMAT`.

```{r}
outcells.pc.bic <- calcbounds(id_ic = id.bic_pc, 
                              IC="bic",
                              res = clustack_pc, 
                              byloc = FALSE,
                              Ex = Ex, 
                              Yx = Yx,
                              cellsix = cells,
                              sparsemat = sparseMAT)
```

All calculations for confidence bounds are performed on the natural log scale. If multiple clusters are identified, then there will be multiple elements to the list, corresponding to each identified cluster. Since in this example only one cluster as identified, we can extract the bounds and other information for this one cluster with `outcells.pc.bic[[1]]`:


We use the `tidyr` and `dplyr` packages to create the data frame and `kableExtra` to format the table.

```{r}
    #Buckland bounds:
outbuck <- cbind(outcells.pc.bic[[1]]$outbuckTlog.theta$buckland.LB,
      outcells.pc.bic[[1]]$outbuckTlog.theta$clusterstack,
      outcells.pc.bic[[1]]$outbuckTlog.theta$buckland.UB) %>%
    as.data.frame() %>%
    mutate(method="Buckland",
           timeperiod = rep(c("Time 1", "Time 3"), each=5),
           cellid = rep(cellsT1, times=2))
#burnham and anderson bounds:
outba <- cbind(outcells.pc.bic[[1]]$outba2Tlog.theta$ba2.LB,
      outcells.pc.bic[[1]]$outba2Tlog.theta$clusterstack,
      outcells.pc.bic[[1]]$outba2Tlog.theta$ba2.UB) %>%
    as.data.frame() %>%
    mutate(method="BurnhamAnderson",
           timeperiod = rep(c("Time 1", "Time 3"), each=5),
           cellid = rep(cellsT1, times=2))
#MATA bounds:
outmata <- cbind(outcells.pc.bic[[1]]$outmataTlog.theta$mata.LB,
      outcells.pc.bic[[1]]$outmataTlog.theta$clusterstack,
      outcells.pc.bic[[1]]$outmataTlog.theta$mata.UB) %>%
    as.data.frame() %>%
    mutate(method="MATA",
           timeperiod = rep(c("Time 1", "Time 3"), each=5),
           cellid = rep(cellsT1, times=2))

out <- rbind.data.frame(outbuck, outba, outmata)

#create table
out %>%
    mutate(across(V1:V3, round,2)) %>%
    mutate(RRstack = V2) %>%
    mutate(CB = paste0("(",V1,",", V3,")")) %>%
    mutate(cellidname = case_when(
        cellid==150 ~ "center",
        cellid==138 ~ "near-center",
        cellid==139 ~ "border-inside",
        cellid==123 ~ "border-outside",
        cellid==55 ~ "far-outside",
    )) %>%
    dplyr::select(-c(V1,V3,V2, cellid)) %>%
    relocate(method,cellidname, timeperiod, method, RRstack, CB ) %>%
    pivot_wider(names_from=c(method), values_from = c(RRstack, CB), id_cols = c(cellidname, timeperiod)) %>%
    dplyr::select(-c(RRstack_BurnhamAnderson, RRstack_MATA)) %>%
    rename(RRstack =  RRstack_Buckland) %>%
    kableExtra::kbl(caption="Clustack Cell-Wise 95 Percent Confidence Bounds (CB) and Stacked Relative Risk Estimates") %>%
    kableExtra::kable_paper("hover", full_width=FALSE)

```

